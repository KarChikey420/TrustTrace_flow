The architecture is built to enforce traceability and zero-hallucination by default. Every ingestion — whether from APIs, CSVs, PDFs, or manual uploads — is fingerprinted using SHA256 and logged with metadata (source, timestamp, format) before being stored in DuckDB or S3. This enables full lineage tracking from raw data to final insight. Data governance is handled via tools like Apache Ranger for RBAC, and Great Expectations for PII flagging and validation. By embedding checks and tagging data with sensitivity levels, we prevent unverified or risky records from surfacing downstream.

Transformations are handled in dbt or PySpark, with built-in assertions that enforce row-level consistency and anomaly detection. Statistical thresholds (e.g., outlier ratios, missing values) gate each model stage, making errors visible early. We use DuckDB for low-cost analytical queries and ChromaDB for semantic search, cross-linked via UUIDs to ensure that vector results are backed by raw records — no hallucinated embeddings or orphaned matches.

To balance cost and scale, the system runs on a hybrid setup: local DuckDB for developer agility, GCS or S3 for scalable storage, and Trino for federated querying if needed. Observability is layered in via Metaplane or OpenMetadata, monitoring schema drift, embedding freshness, and source reliability drops. Fallback logic reverts to the last validated snapshot if confidence scores fall below a threshold.

Schema versioning is handled using DVC or LakeFS, enabling rollback, diff comparisons, and reproducibility. Deduplication happens on ingest via source_fingerprint hashes. Together, these design choices ensure statistical trust, operational resilience, and verifiable insights.
